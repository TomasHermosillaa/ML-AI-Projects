"""
MÃ³dulo de bloques convolucionales para arquitectura U-Net.

Este mÃ³dulo implementa los componentes bÃ¡sicos de construcciÃ³n para redes convolucionales:
- ConvBlock: Bloque bÃ¡sico Conv2d + BatchNorm2d + ReLU
- DoubleConv: Dos convoluciones consecutivas (patrÃ³n estÃ¡ndar U-Net)
- DownBlock: Bloque de reducciÃ³n de resoluciÃ³n
- UpBlock: Bloque de aumento de resoluciÃ³n

Cada bloque estÃ¡ diseÃ±ado para ser modular y reutilizable.

Autor: Proyecto de SegmentaciÃ³n de NÃºcleos
Fecha: 2025
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple


class ConvBlock(nn.Module):
    """
    Bloque convolucional bÃ¡sico: Conv2d â†’ BatchNorm2d â†’ ReLU
    
    Este es el bloque fundamental que se reutiliza en toda la arquitectura U-Net.
    Implementa el patrÃ³n estÃ¡ndar de capas convolucionales modernas.
    
    Componentes:
    - Conv2d: Extrae caracterÃ­sticas mediante filtros aprendibles
    - BatchNorm2d: Normaliza activaciones para entrenamiento estable
    - ReLU: Introduce no-linealidad esencial para patrones complejos
    """
    
    def __init__(self, 
                 in_channels: int,
                 out_channels: int,
                 kernel_size: int = 3,
                 stride: int = 1,
                 padding: int = 1,
                 bias: bool = False):
        """
        Inicializa el bloque convolucional.
        
        Args:
            in_channels: NÃºmero de canales de entrada
            out_channels: NÃºmero de canales de salida  
            kernel_size: TamaÃ±o del filtro convolucional (default: 3)
            stride: Paso de la convoluciÃ³n (default: 1, sin reducciÃ³n)
            padding: Relleno para mantener dimensiones (default: 1)
            bias: Si usar bias en Conv2d (default: False, ya que BatchNorm lo compensa)
        """
        super().__init__()
        
        # Guardar parÃ¡metros para debugging
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        
        # Bloque secuencial: Conv â†’ BatchNorm â†’ ReLU
        self.conv_block = nn.Sequential(
            # Conv2d: El "detector de patrones"
            nn.Conv2d(
                in_channels=in_channels,
                out_channels=out_channels,
                kernel_size=kernel_size,
                stride=stride,
                padding=padding,
                bias=bias  # False porque BatchNorm compensa
            ),
            
            # BatchNorm2d: El "estandarizador"
            nn.BatchNorm2d(out_channels),
            
            # ReLU: El "filtro de positividad"
            nn.ReLU(inplace=True)  # inplace=True para eficiencia de memoria
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        PropagaciÃ³n hacia adelante.
        
        Args:
            x: Tensor de entrada con shape (batch_size, in_channels, height, width)
            
        Returns:
            Tensor de salida con shape (batch_size, out_channels, height, width)
            Nota: height y width se mantienen iguales si padding=1 y stride=1
        """
        return self.conv_block(x)
    
    def __repr__(self) -> str:
        """RepresentaciÃ³n legible para debugging."""
        return (f"ConvBlock(in={self.in_channels}, out={self.out_channels}, "
                f"kernel={self.kernel_size})")


class DoubleConv(nn.Module):
    """
    Bloque de doble convoluciÃ³n: [Conv-BN-ReLU] â†’ [Conv-BN-ReLU]
    
    Este es el patrÃ³n estÃ¡ndar de U-Net original. Cada "bloque" en U-Net
    consiste en dos convoluciones consecutivas, permitiendo que cada nivel
    extraiga caracterÃ­sticas mÃ¡s complejas progresivamente.
    
    Ventajas del patrÃ³n doble:
    - Mayor poder representacional que una sola convoluciÃ³n
    - Permite extraer caracterÃ­sticas mÃ¡s complejas
    - Mantiene resoluciÃ³n espacial constante
    - Es el estÃ¡ndar probado en U-Net
    """
    
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 mid_channels: Optional[int] = None,
                 kernel_size: int = 3,
                 padding: int = 1):
        """
        Inicializa el bloque de doble convoluciÃ³n.
        
        Args:
            in_channels: Canales de entrada
            out_channels: Canales de salida
            mid_channels: Canales intermedios (default: out_channels)
            kernel_size: TamaÃ±o del filtro (default: 3)
            padding: Relleno (default: 1)
        """
        super().__init__()
        
        # Si no se especifica, los canales intermedios = canales de salida
        if mid_channels is None:
            mid_channels = out_channels
        
        # Guardar parÃ¡metros para debugging
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.mid_channels = mid_channels
        
        # Primera convoluciÃ³n: in_channels â†’ mid_channels
        self.conv1 = ConvBlock(
            in_channels=in_channels,
            out_channels=mid_channels,
            kernel_size=kernel_size,
            padding=padding
        )
        
        # Segunda convoluciÃ³n: mid_channels â†’ out_channels
        self.conv2 = ConvBlock(
            in_channels=mid_channels,
            out_channels=out_channels,
            kernel_size=kernel_size,
            padding=padding
        )
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        PropagaciÃ³n hacia adelante: dos convoluciones consecutivas.
        
        Args:
            x: Tensor de entrada (batch_size, in_channels, height, width)
            
        Returns:
            Tensor de salida (batch_size, out_channels, height, width)
            Las dimensiones espaciales (height, width) se mantienen
        """
        x = self.conv1(x)
        x = self.conv2(x)
        return x
    
    def __repr__(self) -> str:
        """RepresentaciÃ³n legible para debugging."""
        return (f"DoubleConv(in={self.in_channels}, mid={self.mid_channels}, "
                f"out={self.out_channels})")


class DownBlock(nn.Module):
    """
    Bloque de downsampling para la parte encoder de U-Net.
    
    Combina:
    1. MaxPool2d: Reduce resoluciÃ³n espacial (256â†’128â†’64â†’32...)
    2. DoubleConv: Extrae caracterÃ­sticas en la nueva resoluciÃ³n
    
    Este patrÃ³n permite que la red capture caracterÃ­sticas a mÃºltiples escalas:
    - Escalas altas: Detalles finos (bordes precisos)
    - Escalas bajas: Patrones globales (forma general de nÃºcleos)
    """
    
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 pool_size: int = 2):
        """
        Inicializa el bloque de downsampling.
        
        Args:
            in_channels: Canales de entrada
            out_channels: Canales de salida
            pool_size: Factor de reducciÃ³n (default: 2, mitad del tamaÃ±o)
        """
        super().__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.pool_size = pool_size
        
        # MaxPool2d: Reduce resoluciÃ³n espacial
        self.pool = nn.MaxPool2d(kernel_size=pool_size, stride=pool_size)
        
        # DoubleConv: Procesa caracterÃ­sticas en nueva resoluciÃ³n
        self.conv = DoubleConv(in_channels, out_channels)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        PropagaciÃ³n hacia adelante: pooling â†’ double convolution.
        
        Args:
            x: Tensor de entrada (batch, in_channels, height, width)
            
        Returns:
            Tensor de salida (batch, out_channels, height//pool_size, width//pool_size)
        """
        x = self.pool(x)
        x = self.conv(x)
        return x
    
    def __repr__(self) -> str:
        return (f"DownBlock(in={self.in_channels}, out={self.out_channels}, "
                f"pool={self.pool_size})")


class UpBlock(nn.Module):
    """
    Bloque de upsampling para la parte decoder de U-Net.
    
    Combina:
    1. ConvTranspose2d: Aumenta resoluciÃ³n espacial (32â†’64â†’128â†’256...)
    2. ConcatenaciÃ³n: Fusiona con skip connection del encoder
    3. DoubleConv: Procesa caracterÃ­sticas fusionadas
    
    Las skip connections son cruciales en segmentaciÃ³n porque:
    - Preservan detalles espaciales finos
    - Permiten localizaciÃ³n precisa de pÃ­xeles
    - Combinan informaciÃ³n de mÃºltiples escalas
    """
    
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 bilinear: bool = False):
        """
        Inicializa el bloque de upsampling.
        
        Args:
            in_channels: Canales de entrada
            out_channels: Canales de salida
            bilinear: Si usar interpolaciÃ³n bilineal en lugar de ConvTranspose2d
        """
        super().__init__()
        
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.bilinear = bilinear
        
        # Upsampling: Aumentar resoluciÃ³n espacial
        if bilinear:
            # OpciÃ³n 1: InterpolaciÃ³n bilinear + Conv1x1
            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)
        else:
            # OpciÃ³n 2: ConvTranspose2d (transposed convolution)
            self.up = nn.ConvTranspose2d(
                in_channels, 
                in_channels // 2, 
                kernel_size=2, 
                stride=2
            )
            self.conv = DoubleConv(in_channels, out_channels)
    
    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
        """
        PropagaciÃ³n hacia adelante con skip connection.
        
        Args:
            x1: Tensor del decoder (resoluciÃ³n baja)
            x2: Tensor del encoder - skip connection (resoluciÃ³n alta)
            
        Returns:
            Tensor fusionado y procesado
        """
        # 1. Upsampling del tensor de resoluciÃ³n baja
        x1 = self.up(x1)
        
        # 2. Ajustar dimensiones si hay diferencias menores
        # (puede ocurrir por diferencias de padding en el encoder)
        diff_y = x2.size()[2] - x1.size()[2]
        diff_x = x2.size()[3] - x1.size()[3]
        
        if diff_y != 0 or diff_x != 0:
            x1 = F.pad(x1, [diff_x // 2, diff_x - diff_x // 2,
                           diff_y // 2, diff_y - diff_y // 2])
        
        # 3. Concatenar a lo largo del canal
        x = torch.cat([x2, x1], dim=1)
        
        # 4. Procesar caracterÃ­sticas fusionadas
        return self.conv(x)
    
    def __repr__(self) -> str:
        return (f"UpBlock(in={self.in_channels}, out={self.out_channels}, "
                f"bilinear={self.bilinear})")


def test_conv_blocks():
    """
    FunciÃ³n de prueba para validar todos los bloques convolucionales.
    """
    print("ðŸ§ª Probando bloques convolucionales...")
    
    # Crear tensor de prueba (simula batch de imÃ¡genes de nÃºcleos)
    batch_size = 2
    test_input = torch.randn(batch_size, 3, 256, 256)
    print(f"Input de prueba: {test_input.shape}")
    
    # 1. Probar ConvBlock bÃ¡sico
    print("\n1. Probando ConvBlock:")
    conv_block = ConvBlock(in_channels=3, out_channels=64)
    output1 = conv_block(test_input)
    print(f"   {conv_block}")
    print(f"   Input: {test_input.shape} â†’ Output: {output1.shape}")
    
    # 2. Probar DoubleConv
    print("\n2. Probando DoubleConv:")
    double_conv = DoubleConv(in_channels=3, out_channels=64)
    output2 = double_conv(test_input)
    print(f"   {double_conv}")
    print(f"   Input: {test_input.shape} â†’ Output: {output2.shape}")
    
    # 3. Probar DownBlock
    print("\n3. Probando DownBlock:")
    down_block = DownBlock(in_channels=3, out_channels=64)
    output3 = down_block(test_input)
    print(f"   {down_block}")
    print(f"   Input: {test_input.shape} â†’ Output: {output3.shape}")
    
    # 4. Probar UpBlock
    print("\n4. Probando UpBlock:")
    # Simular tensores para skip connection
    x1 = torch.randn(batch_size, 128, 64, 64)  # Del decoder
    x2 = torch.randn(batch_size, 64, 128, 128)  # Skip connection del encoder
    
    up_block = UpBlock(in_channels=128, out_channels=64)
    output4 = up_block(x1, x2)
    print(f"   {up_block}")
    print(f"   Decoder: {x1.shape} + Skip: {x2.shape} â†’ Output: {output4.shape}")
    
    print("\nâœ… Todas las pruebas completadas exitosamente!")
    
    return {
        'conv_block': conv_block,
        'double_conv': double_conv, 
        'down_block': down_block,
        'up_block': up_block
    }


if __name__ == "__main__":
    # Ejecutar pruebas cuando se corre directamente
    test_conv_blocks()
